#!/usr/bin/env python3
"""
Quick Model Performance Comparison

Compare baseline vs expanded features performance using a subset of data
for faster results while full training completes in background.
"""

import sys
import pandas as pd
import numpy as np
from pathlib import Path
import logging
import json
import time
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
import xgboost as xgb

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

def quick_comparison():
    """Quick comparison using sample data."""
    print("âš¡ Quick Model Performance Comparison")
    print("=" * 50)
    
    # Load baseline results
    baseline_file = Path('results/model_training/training_results.json')
    if baseline_file.exists():
        with open(baseline_file, 'r') as f:
            baseline_results = json.load(f)
        
        baseline_auc = baseline_results['xgboost']['test_metrics']['roc_auc']
        print(f"ğŸ“Š Baseline XGBoost AUC: {baseline_auc:.4f}")
    else:
        baseline_auc = 0.808  # From our earlier analysis
        print(f"ğŸ“Š Baseline XGBoost AUC: {baseline_auc:.4f} (from logs)")
    
    # Load expanded data (sample for speed)
    try:
        X_train = pd.read_parquet('data/processed_expanded/expanded_X_train_enhanced.parquet')
        y_train = pd.read_parquet('data/processed_enhanced/y_train_enhanced.parquet').iloc[:, 0]
        X_test = pd.read_parquet('data/processed_expanded/expanded_X_test_enhanced.parquet')
        y_test = pd.read_parquet('data/processed_enhanced/y_test_enhanced.parquet').iloc[:, 0]
        
        print(f"ğŸ“‚ Loaded expanded data: {X_train.shape[0]:,} samples, {X_train.shape[1]} features")
        
        # Sample for quick training (10% of data)
        sample_size = min(30000, len(X_train))
        sample_indices = np.random.choice(len(X_train), size=sample_size, replace=False)
        X_train_sample = X_train.iloc[sample_indices].fillna(0)
        y_train_sample = y_train.iloc[sample_indices]
        
        test_sample_size = min(8000, len(X_test))
        test_sample_indices = np.random.choice(len(X_test), size=test_sample_size, replace=False)
        X_test_sample = X_test.iloc[test_sample_indices].fillna(0)
        y_test_sample = y_test.iloc[test_sample_indices]
        
        print(f"ğŸ”¬ Using sample: {len(X_train_sample):,} train, {len(X_test_sample):,} test")
        
        # Quick XGBoost training
        print(f"ğŸš€ Training XGBoost with expanded features...")
        start_time = time.time()
        
        model = xgb.XGBClassifier(
            n_estimators=50,  # Reduced for speed
            max_depth=6,
            learning_rate=0.1,
            random_state=42,
            eval_metric='logloss'
        )
        
        model.fit(X_train_sample, y_train_sample)
        y_pred_proba = model.predict_proba(X_test_sample)[:, 1]
        expanded_auc = roc_auc_score(y_test_sample, y_pred_proba)
        
        training_time = time.time() - start_time
        
        print(f"âœ… Expanded XGBoost AUC: {expanded_auc:.4f}")
        print(f"â±ï¸  Training time: {training_time:.1f}s")
        
        # Calculate improvement
        improvement = expanded_auc - baseline_auc
        improvement_pct = (improvement / baseline_auc) * 100
        
        print(f"\nğŸ“ˆ Performance Comparison:")
        print(f"   Baseline AUC:  {baseline_auc:.4f}")
        print(f"   Expanded AUC:  {expanded_auc:.4f}")
        print(f"   Improvement:   {improvement:+.4f} ({improvement_pct:+.1f}%)")
        
        # Feature importance analysis
        feature_importance = model.feature_importances_
        feature_names = X_train_sample.columns
        
        # Top features
        top_features = sorted(zip(feature_names, feature_importance), 
                            key=lambda x: x[1], reverse=True)[:15]
        
        print(f"\nğŸ” Top 15 Most Important Features:")
        review_features_in_top = 0
        for i, (feature, importance) in enumerate(top_features):
            marker = "ğŸ†•" if feature.startswith('review_') else "ğŸ“Š"
            if feature.startswith('review_'):
                review_features_in_top += 1
            print(f"   {i+1:2d}. {marker} {feature:<35} {importance:.4f}")
        
        print(f"\nğŸ¯ New Review Features in Top 15: {review_features_in_top}")
        
        # Success assessment
        if improvement_pct >= 1.0:
            print(f"\nğŸ‰ SUCCESS! Target improvement achieved ({improvement_pct:+.1f}% > 1%)")
            print(f"   The expanded free review dataset significantly improves model performance!")
        elif improvement_pct >= 0.5:
            print(f"\nğŸ“ˆ GOOD PROGRESS! Moderate improvement ({improvement_pct:+.1f}%)")
            print(f"   With more diverse data sources, we can achieve the 2-5% target.")
        else:
            print(f"\nâš ï¸  NEEDS MORE DATA! Small improvement ({improvement_pct:+.1f}%)")
            print(f"   Consider adding Google Business, OpenStreetMap data for bigger gains.")
        
        return {
            'baseline_auc': baseline_auc,
            'expanded_auc': expanded_auc,
            'improvement': improvement,
            'improvement_pct': improvement_pct,
            'review_features_in_top': review_features_in_top,
            'total_features': len(feature_names),
            'sample_size': len(X_train_sample)
        }
        
    except Exception as e:
        print(f"âŒ Error in quick comparison: {str(e)}")
        return None

def main():
    """Main execution."""
    results = quick_comparison()
    
    if results:
        print(f"\nğŸ’¡ Next Steps Based on Results:")
        
        if results['improvement_pct'] >= 1.0:
            print(f"   âœ… 1. Complete full model training (in progress)")
            print(f"   âœ… 2. Add more external data sources for even bigger gains")
            print(f"   âœ… 3. Deploy enhanced model to production")
        else:
            print(f"   ğŸ”„ 1. Add Google Business Reviews dataset")
            print(f"   ğŸ”„ 2. Integrate OpenStreetMap business data") 
            print(f"   ğŸ”„ 3. Include social media sentiment data")
            print(f"   ğŸ”„ 4. Scale to 1M+ reviews from Kaggle datasets")
        
        print(f"\nğŸ“Š Summary:")
        print(f"   â€¢ Dataset scale: 300 â†’ 37,851 reviews (126x increase)")
        print(f"   â€¢ Business coverage: 10 â†’ 500 businesses (50x increase)")
        print(f"   â€¢ Features: 38 â†’ 66 (+28 sentiment features)")
        print(f"   â€¢ Review features in top 15: {results['review_features_in_top']}")
        print(f"   â€¢ AUC improvement: {results['improvement_pct']:+.1f}%")

if __name__ == "__main__":
    main()