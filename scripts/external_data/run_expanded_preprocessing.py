#!/usr/bin/env python3
"""
Enhanced Preprocessing with Expanded Free Review Datasets

This script processes the expanded free review datasets and integrates them
with loan data for improved model performance.

Key Improvements:
- Processes 1M+ reviews instead of 300 sample reviews
- Multi-dataset sentiment analysis
- Enhanced business name matching with fuzzy logic
- Advanced feature engineering with 25+ sentiment features
- Memory-efficient processing for large datasets

Expected Impact: 2-5% AUC improvement from 0.808 to 0.83-0.85
"""

import sys
import logging
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import time
import pickle
import gc
from typing import Dict, List, Tuple
import argparse

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from src.external_data.free_review_integration import (
    FreeReviewDataProcessor,
    ReviewSentimentAnalyzer, 
    BusinessNameMatcher
)
from src.data_pipeline.preprocessing.enhanced_preprocessor import EnhancedDataPreprocessor

class ExpandedReviewProcessor:
    """Enhanced processor for large-scale free review datasets."""
    
    def __init__(self, data_dir: str = "data/external/free_reviews"):
        self.data_dir = Path(data_dir)
        self.processor = FreeReviewDataProcessor(str(data_dir))
        self.sentiment_analyzer = ReviewSentimentAnalyzer()
        self.name_matcher = BusinessNameMatcher()
        
        self.logger = self._setup_logging()
        
        # Processing configuration
        self.config = {
            'batch_size': 10000,  # Process in batches for memory efficiency
            'max_reviews_per_business': 100,  # Limit reviews per business
            'min_review_length': 10,  # Minimum review text length
            'sentiment_cache_size': 100000,  # Cache sentiment results
            'fuzzy_match_threshold': 0.75,  # Business name matching threshold
            'max_businesses': 50000  # Maximum businesses to process
        }
        
        # Enhanced feature set (25+ features)
        self.feature_names = [
            # Basic metrics (4)
            'review_rating', 'review_count', 'review_categories_count', 'review_data_source_score',
            
            # Sentiment analysis (8)
            'review_avg_polarity', 'review_avg_subjectivity', 'review_vader_compound',
            'review_vader_positive', 'review_vader_negative', 'review_polarity_variance',
            'review_sentiment_trend', 'review_sentiment_volatility',
            
            # Engagement metrics (5)
            'review_text_count', 'review_avg_length', 'review_avg_words',
            'review_engagement_score', 'review_response_rate',
            
            # Risk indicators (4)
            'review_poor_rating', 'review_negative_sentiment', 'review_low_engagement',
            'review_inconsistent_quality',
            
            # Composite scores (4)
            'review_reputation_score', 'review_consistency_score', 'review_trust_score',
            'review_market_position',
            
            # Metadata (3)
            'review_match_confidence', 'review_data_quality', 'review_recency_score'
        ]
    
    def _setup_logging(self):
        """Setup comprehensive logging."""
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            # Console handler
            console_handler = logging.StreamHandler()
            console_handler.setLevel(logging.INFO)
            
            # File handler  
            log_file = self.data_dir.parent.parent.parent / 'expanded_preprocessing.log'
            file_handler = logging.FileHandler(log_file, mode='a')
            file_handler.setLevel(logging.DEBUG)
            
            formatter = logging.Formatter(
                '%(asctime)s - %(levelname)s - %(message)s'
            )
            console_handler.setFormatter(formatter)
            file_handler.setFormatter(formatter)
            
            logger.addHandler(console_handler)
            logger.addHandler(file_handler)
        
        return logger
    
    def load_unified_dataset(self, unified_file: str = "unified_business_reviews.csv") -> bool:
        """
        Load the unified large-scale review dataset.
        
        Args:
            unified_file: Filename of unified dataset
            
        Returns:
            Success status
        """
        unified_path = self.data_dir / unified_file
        
        if not unified_path.exists():
            self.logger.error(f"‚ùå Unified dataset not found: {unified_path}")
            self.logger.info("üí° Run expand_free_datasets.py --create-unified first")
            return False
        
        self.logger.info(f"üìÇ Loading unified dataset: {unified_path}")
        
        try:
            # Load dataset in chunks for memory efficiency
            chunk_size = self.config['batch_size']
            chunks = []
            
            for chunk in pd.read_csv(unified_path, chunksize=chunk_size):
                # Basic filtering
                chunk = chunk[chunk['text'].str.len() >= self.config['min_review_length']]
                chunk = chunk[(chunk['stars'] >= 1) & (chunk['stars'] <= 5)]
                chunks.append(chunk)
                
                # Limit total size
                if len(chunks) * chunk_size >= self.config['max_businesses'] * 20:  # Approx 20 reviews per business
                    break
            
            if not chunks:
                self.logger.error("‚ùå No valid data found in unified dataset")
                return False
            
            # Combine chunks
            unified_df = pd.concat(chunks, ignore_index=True)
            self.logger.info(f"‚úÖ Loaded {len(unified_df):,} reviews from unified dataset")
            
            # Convert to business-level data
            self.business_data = self._create_business_data_from_unified(unified_df)
            
            if self.business_data:
                self.name_matcher.build_business_index(self.business_data)
                self.logger.info(f"‚úÖ Created {len(self.business_data):,} business entries")
                return True
            else:
                return False
                
        except Exception as e:
            self.logger.error(f"‚ùå Failed to load unified dataset: {str(e)}")
            return False
    
    def _create_business_data_from_unified(self, df: pd.DataFrame) -> List:
        """Convert unified review data to business-level data with sentiment analysis."""
        self.logger.info("üîÑ Converting reviews to business-level data with sentiment analysis...")
        
        business_data = []
        sentiment_cache = {}
        
        # Group by business
        business_groups = df.groupby('business_name')
        total_businesses = len(business_groups)
        
        for idx, (business_name, business_reviews) in enumerate(business_groups):
            if idx % 1000 == 0:
                self.logger.info(f"   Progress: {idx:,}/{total_businesses:,} businesses processed")
                gc.collect()  # Memory cleanup
            
            # Limit reviews per business
            if len(business_reviews) > self.config['max_reviews_per_business']:
                business_reviews = business_reviews.sample(
                    n=self.config['max_reviews_per_business'], 
                    random_state=42
                )
            
            # Get business info
            business_info = business_reviews.iloc[0]
            
            # Convert reviews to required format
            reviews = []
            for _, review in business_reviews.iterrows():
                review_text = str(review['text'])
                
                # Use sentiment cache
                text_hash = hash(review_text)
                if text_hash in sentiment_cache:
                    cached_sentiment = sentiment_cache[text_hash]
                else:
                    cached_sentiment = None
                
                reviews.append({
                    'text': review_text,
                    'stars': review['stars'],
                    'date': review.get('date', '2024-01-01'),
                    'review_id': f"review_{idx}_{len(reviews)}",
                    'cached_sentiment': cached_sentiment
                })
            
            # Analyze sentiment (with caching)
            sentiment_metrics = self._analyze_business_sentiment_cached(
                reviews, sentiment_cache
            )
            
            # Create business data entry
            from src.external_data.free_review_integration import BusinessReviewData
            
            business_entry = BusinessReviewData(
                business_id=f"unified_business_{idx}",
                business_name=business_name,
                categories=str(business_info.get('categories', 'Business')).split(','),
                city=str(business_info.get('city', 'Unknown')),
                state=str(business_info.get('state', 'Unknown')),
                rating=float(business_reviews['stars'].mean()),
                review_count=len(business_reviews),
                reviews=reviews,
                sentiment_metrics=sentiment_metrics
            )
            
            business_data.append(business_entry)
            
            # Limit cache size
            if len(sentiment_cache) > self.config['sentiment_cache_size']:
                # Remove oldest entries
                cache_items = list(sentiment_cache.items())
                sentiment_cache = dict(cache_items[-self.config['sentiment_cache_size']//2:])
        
        self.logger.info(f"‚úÖ Completed business-level conversion: {len(business_data):,} businesses")
        return business_data
    
    def _analyze_business_sentiment_cached(self, reviews: List[Dict], cache: Dict) -> Dict:
        """Analyze sentiment with caching for performance."""
        review_sentiments = []
        
        for review in reviews:
            if review.get('cached_sentiment'):
                review_sentiments.append(review['cached_sentiment'])
            else:
                text = review['text']
                if text and len(text) >= self.config['min_review_length']:
                    sentiment = self.sentiment_analyzer.analyze_text_sentiment(text)
                    review_sentiments.append(sentiment)
                    
                    # Cache result
                    text_hash = hash(text)
                    cache[text_hash] = sentiment
        
        if not review_sentiments:
            return self.sentiment_analyzer._empty_sentiment_metrics()
        
        # Enhanced aggregation with additional metrics
        df_sentiments = pd.DataFrame(review_sentiments)
        
        aggregated = {
            # Standard metrics
            'avg_textblob_polarity': df_sentiments['textblob_polarity'].mean(),
            'avg_textblob_subjectivity': df_sentiments['textblob_subjectivity'].mean(),
            'avg_vader_compound': df_sentiments['vader_compound'].mean(),
            'avg_vader_positive': df_sentiments['vader_positive'].mean(),
            'avg_vader_negative': df_sentiments['vader_negative'].mean(),
            
            # Enhanced variance measures
            'polarity_variance': df_sentiments['textblob_polarity'].var(),
            'vader_variance': df_sentiments['vader_compound'].var(),
            
            # Sentiment trend analysis
            'sentiment_trend': self._calculate_sentiment_trend(df_sentiments),
            'sentiment_volatility': self._calculate_sentiment_volatility(df_sentiments),
            
            # Engagement metrics
            'review_count_with_text': len(review_sentiments),
            'avg_text_length': df_sentiments['text_length'].mean(),
            'avg_word_count': df_sentiments['word_count'].mean(),
            'engagement_score': self._calculate_engagement_score(df_sentiments),
            
            # Distribution metrics
            'positive_review_ratio': (df_sentiments['vader_compound'] > 0.05).mean(),
            'negative_review_ratio': (df_sentiments['vader_compound'] < -0.05).mean(),
            'neutral_review_ratio': (abs(df_sentiments['vader_compound']) <= 0.05).mean(),
            
            # Quality indicators
            'quality_inconsistency': self._calculate_quality_inconsistency(df_sentiments),
            'response_rate_proxy': self._calculate_response_rate_proxy(df_sentiments)
        }
        
        return aggregated
    
    def _calculate_sentiment_trend(self, df_sentiments: pd.DataFrame) -> float:
        """Calculate sentiment trend over review sequence."""
        if len(df_sentiments) < 3:
            return 0.0
        
        sentiments = df_sentiments['vader_compound'].values
        x = np.arange(len(sentiments))
        trend = np.polyfit(x, sentiments, 1)[0]  # Linear trend slope
        return float(trend)
    
    def _calculate_sentiment_volatility(self, df_sentiments: pd.DataFrame) -> float:
        """Calculate sentiment volatility (rolling standard deviation)."""
        if len(df_sentiments) < 5:
            return 0.0
        
        sentiments = df_sentiments['vader_compound']
        rolling_std = sentiments.rolling(window=min(5, len(sentiments))).std()
        return float(rolling_std.mean())
    
    def _calculate_engagement_score(self, df_sentiments: pd.DataFrame) -> float:
        """Calculate engagement score based on review length and detail."""
        avg_length = df_sentiments['text_length'].mean()
        avg_words = df_sentiments['word_count'].mean()
        
        # Normalize to 0-1 scale
        length_score = min(avg_length / 500, 1.0)  # 500 chars = full score
        words_score = min(avg_words / 50, 1.0)     # 50 words = full score
        
        return (length_score + words_score) / 2
    
    def _calculate_quality_inconsistency(self, df_sentiments: pd.DataFrame) -> float:
        """Calculate quality inconsistency indicator."""
        if len(df_sentiments) < 3:
            return 0.0
        
        # Measure inconsistency between rating and sentiment
        # Note: This is simplified - in full implementation would compare with actual ratings
        polarity_std = df_sentiments['textblob_polarity'].std()
        return min(polarity_std * 2, 1.0)  # Normalize to 0-1
    
    def _calculate_response_rate_proxy(self, df_sentiments: pd.DataFrame) -> float:
        """Calculate proxy for business response rate based on review patterns."""
        # Simplified proxy - in practice would analyze actual response patterns
        avg_length = df_sentiments['text_length'].mean()
        return min(avg_length / 1000, 1.0)  # Longer reviews might indicate business engagement
    
    def enhance_loan_data_advanced(self, 
                                  loan_df: pd.DataFrame,
                                  business_name_col: str = 'business_name',
                                  location_col: str = 'business_location') -> pd.DataFrame:
        """
        Enhanced loan data processing with advanced matching and features.
        
        Args:
            loan_df: Loan dataset
            business_name_col: Column with business names
            location_col: Column with business locations
            
        Returns:
            Enhanced dataset with 25+ review sentiment features
        """
        if not self.business_data:
            self.logger.error("‚ùå No business data loaded. Load dataset first.")
            return loan_df
        
        self.logger.info(f"üöÄ Enhancing {len(loan_df):,} loan records with advanced sentiment features...")
        
        enhanced_features = []
        match_stats = {'exact': 0, 'fuzzy': 0, 'not_found': 0}
        
        # Process in batches for memory efficiency
        batch_size = self.config['batch_size']
        
        for batch_start in range(0, len(loan_df), batch_size):
            batch_end = min(batch_start + batch_size, len(loan_df))
            batch_df = loan_df.iloc[batch_start:batch_end]
            
            self.logger.info(f"   Processing batch {batch_start:,}-{batch_end:,}")
            
            batch_features = []
            
            for idx, row in batch_df.iterrows():
                business_name = str(row.get(business_name_col, ''))
                location = str(row.get(location_col, ''))
                
                if business_name and business_name != 'nan':
                    # Enhanced business matching
                    matches = self._find_business_matches_advanced(business_name, location)
                    
                    if matches:
                        match_type, best_match, confidence = matches[0]
                        features = self._extract_advanced_features(best_match, confidence, match_type)
                        match_stats[match_type] += 1
                    else:
                        features = self._empty_advanced_features()
                        features['review_data_source_score'] = 0.0
                        features['review_match_confidence'] = 0.0
                        match_stats['not_found'] += 1
                else:
                    features = self._empty_advanced_features()
                    features['review_data_source_score'] = 0.0
                    features['review_match_confidence'] = 0.0
                    match_stats['not_found'] += 1
                
                batch_features.append(features)
            
            enhanced_features.extend(batch_features)
            
            # Memory cleanup
            gc.collect()
        
        # Convert to DataFrame and merge
        features_df = pd.DataFrame(enhanced_features)
        enhanced_df = pd.concat([loan_df.reset_index(drop=True), features_df], axis=1)
        
        # Log statistics
        total_records = len(loan_df)
        match_rate = (match_stats['exact'] + match_stats['fuzzy']) / total_records * 100
        
        self.logger.info(f"‚úÖ Enhanced preprocessing completed:")
        self.logger.info(f"   Total Records: {total_records:,}")
        self.logger.info(f"   Match Rate: {match_rate:.1f}%")
        self.logger.info(f"   Exact Matches: {match_stats['exact']:,}")
        self.logger.info(f"   Fuzzy Matches: {match_stats['fuzzy']:,}")
        self.logger.info(f"   Features Added: {len(self.feature_names)}")
        
        return enhanced_df
    
    def _find_business_matches_advanced(self, business_name: str, location: str) -> List[Tuple]:
        """Advanced business matching with confidence scoring."""
        if not self.name_matcher.business_index:
            return []
        
        normalized_query = self.name_matcher.normalize_business_name(business_name)
        matches = []
        
        # Exact match
        if normalized_query in self.name_matcher.business_index:
            exact_matches = self.name_matcher.business_index[normalized_query]
            if isinstance(exact_matches, list):
                for match in exact_matches:
                    matches.append(('exact', match, 1.0))
            else:
                matches.append(('exact', exact_matches, 1.0))
        
        # Fuzzy matching with enhanced scoring
        if not matches:
            from difflib import SequenceMatcher
            
            fuzzy_candidates = []
            
            for indexed_name, businesses in self.name_matcher.business_index.items():
                if isinstance(businesses, list):
                    similarity = SequenceMatcher(None, normalized_query, indexed_name).ratio()
                    if similarity >= self.config['fuzzy_match_threshold']:
                        for business in businesses:
                            fuzzy_candidates.append((similarity, business))
            
            # Sort by similarity and add to matches
            fuzzy_candidates.sort(key=lambda x: x[0], reverse=True)
            for similarity, business in fuzzy_candidates[:3]:  # Top 3 fuzzy matches
                matches.append(('fuzzy', business, similarity))
        
        # Location-based filtering (if provided)
        if location and location != 'nan' and matches:
            location_scored = []
            location_normalized = location.lower()
            
            for match_type, business, confidence in matches:
                location_boost = 0.0
                if (location_normalized in business.city.lower() or 
                    location_normalized in business.state.lower()):
                    location_boost = 0.1
                
                adjusted_confidence = min(confidence + location_boost, 1.0)
                location_scored.append((match_type, business, adjusted_confidence))
            
            matches = location_scored
        
        return matches[:1]  # Return best match only
    
    def _extract_advanced_features(self, business, confidence: float, match_type: str) -> Dict[str, float]:
        """Extract comprehensive feature set from business data."""
        sentiment = business.sentiment_metrics
        
        features = {
            # Basic business metrics (4)
            'review_rating': business.rating,
            'review_count': business.review_count,
            'review_categories_count': len(business.categories),
            'review_data_source_score': self._calculate_data_source_score(business),
            
            # Enhanced sentiment analysis (8)
            'review_avg_polarity': sentiment.get('avg_textblob_polarity', 0.0),
            'review_avg_subjectivity': sentiment.get('avg_textblob_subjectivity', 0.0),
            'review_vader_compound': sentiment.get('avg_vader_compound', 0.0),
            'review_vader_positive': sentiment.get('avg_vader_positive', 0.0),
            'review_vader_negative': sentiment.get('avg_vader_negative', 0.0),
            'review_polarity_variance': sentiment.get('polarity_variance', 0.0),
            'review_sentiment_trend': sentiment.get('sentiment_trend', 0.0),
            'review_sentiment_volatility': sentiment.get('sentiment_volatility', 0.0),
            
            # Enhanced engagement metrics (5)
            'review_text_count': sentiment.get('review_count_with_text', 0),
            'review_avg_length': sentiment.get('avg_text_length', 0.0),
            'review_avg_words': sentiment.get('avg_word_count', 0.0),
            'review_engagement_score': sentiment.get('engagement_score', 0.0),
            'review_response_rate': sentiment.get('response_rate_proxy', 0.0),
            
            # Enhanced risk indicators (4)
            'review_poor_rating': 1 if business.rating < 3.0 else 0,
            'review_negative_sentiment': 1 if sentiment.get('avg_vader_compound', 0.0) < -0.1 else 0,
            'review_low_engagement': 1 if sentiment.get('review_count_with_text', 0) < 10 else 0,
            'review_inconsistent_quality': sentiment.get('quality_inconsistency', 0.0),
            
            # Enhanced composite scores (4)
            'review_reputation_score': self._calculate_enhanced_reputation_score(business, sentiment),
            'review_consistency_score': self._calculate_enhanced_consistency_score(sentiment),
            'review_trust_score': self._calculate_trust_score(business, sentiment),
            'review_market_position': self._calculate_market_position_score(business, sentiment),
            
            # Enhanced metadata (3)
            'review_match_confidence': confidence,
            'review_data_quality': self._calculate_data_quality_score(sentiment),
            'review_recency_score': self._calculate_recency_score(business)
        }
        
        return features
    
    def _empty_advanced_features(self) -> Dict[str, float]:
        """Return empty advanced feature set."""
        return {name: 0.0 for name in self.feature_names}
    
    def _calculate_data_source_score(self, business) -> float:
        """Calculate data source quality score."""
        # Score based on review count and data completeness
        review_score = min(business.review_count / 50, 1.0)
        completeness_score = len([cat for cat in business.categories if cat.strip()]) / 5
        return (review_score + min(completeness_score, 1.0)) / 2
    
    def _calculate_enhanced_reputation_score(self, business, sentiment: Dict) -> float:
        """Enhanced reputation score with multiple factors."""
        rating_component = business.rating / 5.0 if business.rating > 0 else 0
        sentiment_component = (sentiment.get('avg_vader_compound', 0.0) + 1) / 2
        consistency_component = 1 - sentiment.get('sentiment_volatility', 0.0)
        
        # Weight by review count and engagement
        review_weight = min(business.review_count / 100, 1.0)
        engagement_weight = sentiment.get('engagement_score', 0.0)
        
        base_score = (rating_component * 0.4 + sentiment_component * 0.4 + consistency_component * 0.2)
        weighted_score = base_score * (0.7 + 0.15 * review_weight + 0.15 * engagement_weight)
        
        return min(weighted_score, 1.0)
    
    def _calculate_enhanced_consistency_score(self, sentiment: Dict) -> float:
        """Enhanced consistency score with trend analysis."""
        variance_component = 1 - sentiment.get('sentiment_volatility', 0.0)
        trend_stability = 1 - abs(sentiment.get('sentiment_trend', 0.0))
        
        return (variance_component + trend_stability) / 2
    
    def _calculate_trust_score(self, business, sentiment: Dict) -> float:
        """Calculate business trust score."""
        # Combine multiple trust indicators
        review_volume_trust = min(business.review_count / 100, 1.0)
        sentiment_trust = (sentiment.get('avg_vader_compound', 0.0) + 1) / 2
        consistency_trust = self._calculate_enhanced_consistency_score(sentiment)
        engagement_trust = sentiment.get('engagement_score', 0.0)
        
        return (review_volume_trust + sentiment_trust + consistency_trust + engagement_trust) / 4
    
    def _calculate_market_position_score(self, business, sentiment: Dict) -> float:
        """Calculate relative market position score."""
        # Simplified market position based on rating and review volume
        rating_position = business.rating / 5.0
        volume_position = min(business.review_count / 200, 1.0)  # 200+ reviews = top position
        
        return (rating_position + volume_position) / 2
    
    def _calculate_data_quality_score(self, sentiment: Dict) -> float:
        """Calculate data quality score."""
        text_coverage = min(sentiment.get('review_count_with_text', 0) / 20, 1.0)
        avg_length_quality = min(sentiment.get('avg_text_length', 0) / 200, 1.0)
        
        return (text_coverage + avg_length_quality) / 2
    
    def _calculate_recency_score(self, business) -> float:
        """Calculate recency score (simplified for demo)."""
        # In full implementation, would analyze review dates
        # For now, use review count as proxy for recent activity
        return min(business.review_count / 50, 1.0)

def main():
    """Main execution function."""
    parser = argparse.ArgumentParser(
        description="Enhanced Preprocessing with Expanded Review Datasets"
    )
    parser.add_argument(
        '--load-unified',
        action='store_true',
        help='Load unified dataset and process business data'
    )
    parser.add_argument(
        '--enhance-loans',
        action='store_true',
        help='Enhance loan data with expanded review features'
    )
    parser.add_argument(
        '--run-full-pipeline',
        action='store_true',
        help='Run complete preprocessing pipeline'
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default='data/processed_expanded',
        help='Output directory for processed data'
    )
    
    args = parser.parse_args()
    
    # Initialize processor
    processor = ExpandedReviewProcessor()
    
    if args.load_unified or args.run_full_pipeline:
        print("üìÇ Loading unified dataset...")
        success = processor.load_unified_dataset()
        if not success:
            print("‚ùå Failed to load unified dataset")
            return
        print("‚úÖ Unified dataset loaded successfully")
    
    if args.enhance_loans or args.run_full_pipeline:
        print("\nüöÄ Enhancing loan data with expanded features...")
        
        # Load existing loan data
        loan_files = [
            'data/processed_enhanced/X_train_enhanced.parquet',
            'data/processed_enhanced/X_val_enhanced.parquet', 
            'data/processed_enhanced/X_test_enhanced.parquet'
        ]
        
        output_dir = Path(args.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        for loan_file in loan_files:
            loan_path = Path(loan_file)
            if loan_path.exists():
                print(f"   Processing: {loan_path.name}")
                
                # Load loan data
                loan_df = pd.read_parquet(loan_path)
                print(f"     Loaded: {len(loan_df):,} records")
                
                # Enhance with expanded features
                enhanced_df = processor.enhance_loan_data_advanced(loan_df)
                
                # Save enhanced data
                output_file = output_dir / f"expanded_{loan_path.name}"
                enhanced_df.to_parquet(output_file, index=False)
                
                print(f"     ‚úÖ Saved enhanced data: {output_file}")
                print(f"     Features added: {len(processor.feature_names)}")
            else:
                print(f"   ‚ö†Ô∏è  Skipping missing file: {loan_file}")
        
        print(f"\n‚úÖ Enhanced preprocessing completed!")
        print(f"   Output directory: {output_dir}")
        print(f"   Total features added: {len(processor.feature_names)}")
        print(f"   Expected AUC improvement: +2-5% (0.808 ‚Üí 0.83-0.85)")

if __name__ == "__main__":
    main()